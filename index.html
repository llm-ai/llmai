<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>LLM-AI 2023</title>

    <!-- favicon -->
    <link href="https://github.com/fluidicon.png" rel=icon>

    <!-- web-fonts -->
    <link href='https://fonts.googleapis.com/css?family=Roboto:100,300,400,700,500' rel='stylesheet' type='text/css'>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Style CSS -->
    <link href="css/style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>

    <![endif]-->
</head>
<body id="page-top" data-spy="scroll" data-target=".navbar">
<div id="main-wrapper">
    <!-- Page Preloader -->
    <div id="preloader">
        <div id="status">
            <div class="status-mes"></div>
        </div>
    </div>

    <header class="header">
        <!-- Navigation -->
        <nav class="navbar main-menu" role="navigation">
            <div class="container">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <!--
                        <a class="navbar-brand page-scroll" href="#page-top"><img src="img/logo.png" alt=""></a>
                    -->
                </div>

                <!-- Collect the nav links, forms, and other content for toggling -->
                <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                    <ul class="nav navbar-nav">
                        <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                        <li class="hidden"><a href="#page-top"></a></li>
                        <li><a class="page-scroll" href="#section-intro">About</a></li>
                         <li><a class="page-scroll" href="#cfp">CFP</a></li> 
                        <li><a class="page-scroll" href="#section-invited">Invited Speakers</a></li>
                        <li><a class="page-scroll" href="#papers">Accepted Papers</a></li>
			    <li><a class="page-scroll" href="#Schedule">Schedule</a></li>
                        <li><a class="page-scroll" href="#section-organizers">ORGANIZERS</a></li>
			 <li><a class="page-scroll" href="#pc">Program Committe</a></li>
<!--                        <li><a class="page-scroll" href="#section-ajenda">Agenda</a></li>-->
<!--                        <li><a class="page-scroll" href="#section-dates">Important Dates</a></li>-->

                    </ul>
                </div>
                <!-- /.navbar-collapse -->
            </div>
            <!-- /.container -->
        </nav>

        <!-- .nav -->

    </header>

    <div class="jumbotron text-center">
        <div class="content">
            <div class="event-date">August 7th, 2023</div>
            <h1> Foundations and Applications in Large-scale AI Models
                 <br>-Pre-training, Fine-tuning, and Prompt-based Learning</br></h1>
            <p class="lead">Workshop held in conjunction with <a href="https://kdd.org/kdd2023/" "class="button scrolly"> KDD 2023 </a></p>
            <a href="#section-intro"><button type="button" class="btn btn-default btn-lg" >See Details</button></a>
        </div>
    </div>
    <!-- .Jumbotron-->


    <section id="section-intro" class="section-wrapper about-event gray-bg ">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <p class="lead">
Deep learning techniques have advanced rapidly in recent years, leading to significant progress in pre-trained and fine-tuned large-scale AI models. For example, in the natural language processing domain, the traditional "pre-train, fine-tune" paradigm is shifting towards the "pre-train, prompt, and predict" paradigm, which has achieved great success on many tasks across different application domains such as ChatGPT/BARD for Conversational AI and P5 for a unified recommendation system. Moreover, there has been a growing interest in models that combine vision and language modalities (vision-language models) which are applied to tasks like Visual Captioning/Generation.		   
		    </p>    
                    <p class="lead" >
Considering the recent technological revolution, it is essential to have a workshop at the KDD conference that emphasizes these paradigm shifts and highlights the paradigms with the potential to solve different tasks. This workshop will provide a platform for academic and industrial researchers to showcase their latest work, share research ideas, discuss various challenges, and identify areas where further research is needed in pre-training, fine-tuning, and prompt-learning methods for large-scale AI models. The workshop will also foster the development of a strong research community focused on solving challenges related to large-scale AI models, providing superior and impactful strategies that can change people’s lives in the future.		
		    </p>  
		    
		    <p class="lead">
            
We invite submissions of long (eight papers) and short (four pages) papers, representing original research, preliminary research results, and proposals for new work in academia or industry. All submissions will be single-blind and will be peer-reviewed by an international program committee of researchers and industrial professionals and experts. 

Accepted submissions will be required to be presented at the workshop and will be published in a dedicated workshop proceeding by the workshop organisers. 


		</p>    
               <p class="lead" >
			Topics of interest in this workshop include but are not limited to: 
		<p> <b> Pre-training:</b> </p>
		<p>- Improvements in pre-training:
supervised pre-training, self-supervised pre-training with various auxiliary tasks, meta-learning, prompt-based Learning, multi-modal pre-training etc. </p>
		<p>- Novel pre-training methods to maximize generalization </p>
		<p>- Model selection for pre-trained models </p>
		<p>- Pre-training for various application domains, such as computer vision, natural language processing, robotics, etc </p>
		<p>  <b> Fine-tuning:</b></p>
		<p>- Domain/task adaptive fine-tuning </p>
		<p>- Intermediate-task, multi-task, self-supervised, MLM fine-tuning</p>
		<p>- Parameter-efficient fine-tuning: sparse parameter tuning, pruning</p>
		<p>- Text-to-Text, Text-to-image, Image-to-text, multi-modal fine-tuning, effectively using large autoregressive pre-trained models</p>
		<p>- Fine-tuning for various application domains, such as computer vision, natural language processing, robotics, etc</p>    
		<p> <b> Prompted/Instruction-based:</b></p>
		<p>- Manual Template Engineering</p>
		<p>- Automated Template Learning</p>
		<p>- Multi-Prompt Learning; Multi-tasks instruction tuning </p>		
		<p>- Instruction tuning with HF/RLHF</p>
		<p>- chain-of-thought (CoT) prompting</p>
		<p> <b> Performance:</b></p>
		<p>- Model compression techniques </p>
		<p>- Large-scale model deployments</p>
		<p>- Efficient and effective training/inference  </p>		
		<p>- Empirical analysis of various pre-training and fine-tuning methods</p>
		<p>- Generalization bounds of different pre-training and fine-tuning methods</p>
		<p>- Stability, sparsity and robustness strategies</p>
		<p> <b> Downstream tasks of large-scale models:</b></p>    
		<p>- NLP models for Text Generation,Text Summarization,Question Answering and other downstream tasks </p>
		 <p>-CV models for Image Captioning, Semantic Segmentation,Object Tracking and other downstream tasks </p>
	        <p> <b> Applications powered by large-scale models:</b></p> 
		<p>-Conversational AI, Conversational Chatbots</p>
		<p>- Enhanced Web Search, Search Engine</p> 
		<p>- Unified, Personalized next generation recommender systems</p> 
			
			 
                </div>
            </div>
        </div>
    </section>
		    
<section id="cfp" class="section-wrapper team white-bg">
		<div class="container">
		   <div class="row">
                <div class="col-md-12">
                    <div class="section-title">
                        <h1>Call for Papers</h1>
                    </div>

	<p><strong> Paper Submission Deadline: June 16, 2023, 11:59 PM AoE. </strong></p>
       <p><strong> Paper Notification: Jun. 26, 2023, 11:59 PM AoE.</strong></p>
	<p><strong>Camera Ready Version: July. 15, 2023, 11:59 PM AoE.</strong></p>
       <p><strong>Half-Day Workshop: Aug. 7, 2023</strong></p>	  
		    
<p>This workshop follows the submission requirement by KDD.		</p>
<p>Instructions:</p>
   <p> - Long paper (up to 8 pages) and short paper (up to 4 pages). The page limit includes the bibliography and any possible appendices.</p>
   <p> -  Single-blind peer review </p>
   <p> - All papers must be formatted according to ACM sigconf template manuscript style, following the submission guidelines available at: https://www.acm.org/publications/proceedings-template.</p>
   <p> - Papers should be submitted in PDF format, electronically, using the  
			<a href="https://easychair.org/conferences/?conf=llm4ai" "class="button scrolly">EasyChair submission system</a> </p>
   <p> - All selected papers will invited for presentation.</p>
			<p> Inquiry Email: llmai.workshop@gmail.com </p>
		   </div>
			   </div>
            </div>
</section>		    

<section id="section-invited" class="section-invited gray-bg">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <div class="section-title">
                        <h1>Invited Speakers</h1>
                    </div>
                </div>
            </div>
		 </div>

	<div class="container">	
		<div class="row" id="section-invited-1">
               		<div class="col-sm-0 col-md-4">
		       		<figure class="thumbnail">
                        		<img src="img/Ed.jpeg" class="img-responsive" alt="Image"></a>
                    		</figure>
                 	</div>
                

                	<div class="col-sm-0 col-md-8">
                   		<div class="section-title">
<!--                         		<h3 align="left">Beyond Being Accurate: Neural Modeling and Reinforcement Learning for Large-Scale Real-World Recommendation Problems

</h3> -->
					<p></p>
					<h4  align="left">Ed H. Chi</a></h4>
                        		<h4  align="left"><small>Distinguished Scientist at Google</small></h4>
					<p></p>
<!--                         		<h5 align="justify"> 
						
			              Fundamental improvements in recommendation and ranking have been much harder to come by, when compared with recent progress on other long-standing AI problems such as visual/audio machine perception and machine translation. 
						Some reasons include: (1) large amounts of data making training difficult, yet having (2) noisy and sparse labels; (3) changing dynamics of context such as user preferences and items; and (4) low-latency requirement for a system response. Moreover, one new challenge is devising approaches to (5) learning more inclusive and robust models.

<br>
<br>
In this talk, Ed will touch upon many recent advances in neural modeling techniques for recommendations and their impact in Google products covering ~600 improvements over the last 5 years across these problems, including:

<ul>
  <li> (a). Multi-task models with gated mixture of experts and utilizing TPUs for large sparse models</li>
  <li> (b). Large output item spaces with Neural Deep Retrieval</li>
  <li> (c). Policy gradient RL techniques with off-policy correction in recommendation models</li>
	<li> (d). Diversification and slate optimization</li>
	<li> (e). Adversarial approaches for inclusiveness and robustness for Classifiers and Recommenders</li>
</ul> 

				</h5> -->
					<p></p>
					<h5 align="justify" >Speaker Bio:
						<br>
Ed H. Chi is a Distinguished Scientist at Google, leading several machine learning research teams focusing on neural modeling, reinforcement learning, dialog chatbot models called LaMDA, reliable/robust machine learning, and recommendation systems in Google Brain team. His team has delivered significant improvements for YouTube, News, Ads, Google Play Store at Google with ~600 product improvements since 2013. With 39 patents and >150 research articles, he is also known for research on user behavior in web and social media.


Prior to Google, he was the Area Manager and a Principal Scientist at Palo Alto Research Center's Augmented Social Cognition Group, where he led the team in understanding how social systems help groups of people to remember, think and reason. Ed completed his three degrees (B.S., M.S., and Ph.D.) in 6.5 years from University of Minnesota. Recognized as an ACM Distinguished Scientist and elected into the CHI Academy, he recently received a 20-year Test of Time award for research in information visualization. He has been featured and quoted in the press, including the Economist, Time Magazine, LA Times, and the Associated Press. An avid swimmer, photographer and snowboarder in his spare time, he also has a blackbelt in Taekwondo.
						</br>

				</h5>
				</div>
                    		</div>
                	</div>
            	</div>
	
	
		<div class="row" id="section-invited-1-bio">
                	<div class="col-sm-0 col-md-6">
                    		<div class="section-title">
                        		<h3></h3>
                        		
					
                    		</div>
                	</div>
			<div class="col-sm-0 col-md-6">
                    		<div class="section-title">
					<h3></h3>
                        		
                	</div>
		</div>
	</div>
	
<div class="container">	
		<div class="row" id="section-invited-1">
               		<div class="col-sm-0 col-md-4">
		       		<figure class="thumbnail">
                        		<img src="img/tania.jpeg" class="img-responsive" alt="Image"></a>
                    		</figure>
                 	</div>
                

                	<div class="col-sm-0 col-md-8">
                   		<div class="section-title">
<!--                         		<h3 align="left">A Path Towards More Universal Question Answering Systems</h3> -->
					<p></p>
					<h4  align="left">Tania Bedrax-Weiss </a></h4>
                        		<h4  align="left"><small>Director of Research at Google</small></h4>
					<p></p>
<!--                         		<h5 align="justify"> 
					With the growing capabilities in how we represent and process structured and unstructured text, there are many opportunities to innovate newer and deeper search paradigms. This would allow us to go beyond building one-off models, each focusing on a specific question-answering (QA) task targeting a specific domain, reasoning skill, or answer type. In this talk, Caiming will start with a series of in-depth presentations on a few of the recent QA research conducted by the Salesforce AI Research team addressing several key questions in the QA domain.
				</h5> -->
					<p></p>
					<h5 align="justify">Speaker Bio:
						<br>
Tania Bedrax Weiss is a Director of Research at Google. Her current focus is on identifying untapped problems at the intersection of Recommender Systems, Natural Language Understanding, and Vision at Google and outlining research agendas to advance the state-of-the-art. During the 15+ years she’s been at Google she has launched transformative products in Google Play, Ads, and Search. Previously, she worked at NASA Ames Research Center and was part of the team that wrote the software used to schedule daily observations for Spirit and Opportunity. She has also worked in industry on automated configuration and pricing systems. She holds a PhD from the University of Oregon in Artificial Intelligence.
					</br>
				</h5>
				</div>
                    		</div>
                	</div>
            	</div>
	
	
	
		<div class="row" id="section-invited-1-bio">
                	<div class="col-sm-0 col-md-6">
                    		<div class="section-title">
                        		<h3></h3>
                        		
					
                    		</div>
                	</div>
			<div class="col-sm-0 col-md-6">
                    		<div class="section-title">
					<h3></h3>
                        		
                	</div>
		</div>
	</div>


	<div class="container">	
		<div class="row" id="section-invited-1">
               		<div class="col-sm-0 col-md-4">
		       		<figure class="thumbnail">
                        		<img src="img/shafiq.png" class="img-responsive" alt="Image"></a>
                    		</figure>
                 	</div>
                

                	<div class="col-sm-0 col-md-8">
                   		<div class="section-title">
<!--                         		<h3 align="left">Responsible Recommendation of Information Items: Veracity, Fairness, and Trust</h3> -->
					<p></p>
					<h4  align="left">Shafiq Joty</a></h4>
                        		<h4  align="left"><small>Research Director at Salesforce AI Research</small></h4>
					<p></p>
<!--                        		<h5 align="justify">Recommender systems have grown beyond E-commerce applications to novel domains.  In particular, information items such as news articles, social media posts and reviews are complex items associated with properties such as author’s viewpoints, stance, reputation, credibility, and bias. We posit that the objective of recommendation of such complex information items shall go beyond high accuracy of satisfying user personal interests to high social responsibility.   Socially responsible recommender systems  call for shift to a new paradigm of data preparation, recommendation model and evaluation.  In this talk, I will discuss our recent work on recommendation of social media items for the mitigation of misinformation and fairness of information sources, as well as user trust of information items. 
				</h5> -->
					<p></p> 
					<h5 align="justify">Speaker Bio:
						<br>
	Shafiq Joty is currently a research director at Salesforce Research (Palo Alto, USA), where he directs the NLP group's work on large language modeling (LLM) and generative AI. He is also a tenured Associate Professor (currently on leave) in the School of Computer Science and Engineering (SCSE) at NTU. He was a founding manager of the Salesforce Research Asia (Singapore) lab. His research contributed to 20+ patents and more than 120+ papers in top-tier NLP and ML conferences and journals. He severed as a PC chair of SIGDIAL-2023, best paper award committee of ICLR-23, NAACL-22 and a (senior) area chair for all the NLP and ML conferences.                 		</br>
				</h5>
				</div>
                    		</div>
                	</div>
            	</div>
	
	
		<div class="row" id="section-invited-1-bio">
                	<div class="col-sm-0 col-md-6">
                    		<div class="section-title">
                        		<h3></h3>
                        		
					
                    		</div>
                	</div>
			<div class="col-sm-0 col-md-6">
                    		<div class="section-title">
					<h3></h3>
                        		
                	</div>
		</div>
	</div>
	
		<div class="container">	
		<div class="row" id="section-invited-1">
               		<div class="col-sm-0 col-md-4">
		       		<figure class="thumbnail">
                        		<img src="img/shen.jpeg" class="img-responsive" alt="Image"></a>
                    		</figure>
                 	</div>
                

                	<div class="col-sm-0 col-md-8">
                   		<div class="section-title">
<!--                         		<h3 align="left">Scaling, Strengthening and Serving: Innovating through Large-Scale Deep Representations in E-commerce Product Search and Recommendation Applications</h3>
					<p></p> -->
					<h4  align="left">Yikang Shen</a></h4>
                        		<h4  align="left"><small> Research Staff Member at MIT-IBM Watson Lab
					</small></h4>
					<p></p>
<!--                         		<h5 align="justify">
					Recent work in e-commerce product search and recommendations have shown result improvement by using large scale pre-trained model based Deep Learning approach. However, this has not been an easy task in the real-world development and deployment due to several constraints: 1) scaling the size of model and training data with computational efficiency is challenging; 2) boosting the incumbent ML applications performance using large scale pre-trained model in the real business setting requires careful algorithm design and data strategy; and 3) serving millions of requests per second at high throughput and low latency is a daunting task. In this talk, I will share our recent work that addresses these issues and a vision for future innovations.
				</h5> -->
					<p></p>
                        		<h5 align="left">
					</h5>
					<p></p>
					<h5 align="justify">Speaker Bio:
						<br>
			Yikang Shen is a Research Staff Member at MIT-IBM Watson Lab. He obtained his Ph.D. from Mila lab at the University of Montreal, where he worked with Aaron Courville. During his Ph.D., his research focused on studying the fundamental mechanisms that allow deep learning methods to understand the latent structure of human language and model its compositional phenomenons. He received the ICLR best paper award in 2019 for his work on inducing grammar from language modeling. After joining IBM, he worked on building modular foundation models that are flexible, efficient, and extendable. He is also interested in developing instruction-tuning and alignment methods that require minimum human annotation. He is now leading IBM's effort to train a large-scale modular language model.	
					</br>
				</h5>
				</div>
                    		</div>
                	</div>
            	</div>


	</div>
		   

    </section>

<section id="paper" class="section-wrapper team white-bg">
  <div class="container">
      <div class="row">
           <div class="col-md-12">
               <div class="section-title">
                        <h1>Accepted Papers</h1>
                 </div>
		 <p>- Retrieval-Augmented Multimodal Language Modeling by Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer and Wen-Tau Yih
		 </p>
		<p>- AutoHint: Automatic Prompt Optimization with Hint Generation by Hong Sun, Xue Li, Yinchuan Xu, Youkow Homma, Qi Cao, Min Wu, Jian Jiao and Denis Charles
		</p>
		<p>- Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation by Zhichao Wang, Mengyu Dai and Keld Lundgaard
		 </p>
		<p>- Compositional Prompting with Successive Decomposition for Multimodal Language Models by Long Hoang Dang, Thao Minh Le, Tu Minh Phuong and Truyen Tran
		</p>
		 <p>- Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation,Zhen Guo, Yanwei Wang, Peiqi Wang and Shangdi Yu.
		</p>
		<p>- In-Context Learning User Simulators for Task-Oriented Dialog Systems by Silvia Terragni, Modestas Filipavicius, Nghia Khau, Bruna Guedes, André Manso and Roland Mathis
	
		</p>
		<p>- Challenges in post-training quantization of Vision Transformers by Piotr Kluska, Florian Scheidegger, A. Cristano I. Malossi and Enrique S. Quintana-Ortí
		</p>
		<p>-Extractive Summarization via ChatGPT for Faithful Summary Generation by Haopeng Zhang, Xiao Liu and Jiawei Zhang
		</p>
		<p>- Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion by Haotian Ju, Dongyue Li, Aneesh Sharma and Hongyang Zhang
		   

      </div>
     </div>
      </div>
</section>

<section id="Schedule" class="section-wrapper team white-bg">
  <div class="container">
      <div class="row">
           <div class="col-md-12">
               <div class="section-title">
                        <h1>Workshop Program</h1>
                 </div>

          <style type="text/css">
                    .tftable {
                        font-size: 15px;
                        color: #333333;
                        width: 100%;
                        border-width: 1px;
                        border-color: #729ea5;
                        border-collapse: collapse;
                    }

                    .tftable th {
                        font-size: 15px;
                        background-color: #acc8cc;
                        border-width: 1px;
                        padding: 8px;
                        border-style: solid;
                        border-color: #729ea5;
                        text-align: left;
                    }

                    .tftable tr {
                        background-color: #d4e3e5;
                    }

                    .tftable td {
                        font-size: 15px;
                        border-width: 1px;
                        padding: 8px;
                        border-style: solid;
                        border-color: #729ea5;
                    }

                    .tftable tr:hover {
                        background-color: #ffffff;
                    }
                </style>
		

        <table class="tftable" border="1">
                    <tr>
                        <th>Time</th>
                        <th>Speaker</th>
                        <th>Title</th>
                    </tr>

                    <tr>
                        <td>8:00-8:05AM, 2023/08/07 (PDT)
                        </td>
                        <td>Host Chair</td>
                        <td>Welcome and Open Remarks</td>
                    </tr>

                    <tr>
                        <td>8:05-8:35AM, 2023/08/07 (PDT) </td>
                        <td>Ed Chi [Google]</td>
                        <td>Talk 1: </td>
                    </tr>
                    <tr>
                        <td>8:35-9:05AM, 2023/08/07 (PDT) </td>
                        <td>Tania Bedrax-Weiss [Google]</td>
                        <td> Talk 2: 
			</td>
                    </tr>
                    <tr>
                        <td>9:05-9:20AM, 2023/08/07 (PDT) </td>
                        <td> Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer and Wen-Tau Yih </td>
                        <td> Paper-1 : Retrieval-Augmented Multimodal Language Modeling</td>
                    
                    </tr>
                    <tr>
                      <td>9:20-9:35AM, 2023/08/07 (PDT) </td>
                        <td>Silvia Terragni, Modestas Filipavicius, Nghia Khau, Bruna Guedes, André Manso and Roland Mathis</td>
                        <td> Paper-2: In-Context Learning User Simulators for Task-Oriented Dialog Systems
</td>
                    </tr>
                    <tr>
                        <td>9:35-9:50AM, 2023/08/07 (PDT)</td>
			<td>Piotr Kluska, Florian Scheidegger, A. Cristano I. Malossi and Enrique S. Quintana-Ortí</td>
			<td> Paper-3 : Challenges in post-training quantization of Vision Transformers/td>     
                    </tr>
		     <tr>
                        <td>9:50-10:05AM, 2023/08/07 (PDT)</td>
			<td>Haotian Ju, Dongyue Li, Aneesh Sharma and Hongyang Zhang</td>
			<td> Paper-4 : Generalization in Graph Neural Networks: Improved PAC-Bayesian Bounds on Graph Diffusion/td> 
                    </tr>
		     <tr>
                        <td>10:05-10:25AM, 2023/08/07 (PDT)
			<td  style="text-align:center" colspan="2">Coffee Break</td>
                    </tr>
                    
                    <tr>
                         <td>10:25-10:55AM, 2023/08/07 (PDT) </td>
                        <td> Shafiq Joty [Salesforce]</td>
                        <td> Talk 3: NLP Research in the Era of LLMs</td>
               
                    </tr>
                    <tr>       
                         <td>10:55-11:25AM, 2023/08/07 (PDT) </td>
                        <td> YiKang Shen[IBM]</td>
                        <td> Talk 4: Modular Large Language Model and Principle-Driven alignment with Minimal Human Supervision </td>
                
                    </tr>
                    <tr>
                        
                         <td>11:25 - 11:35AM, 2023/08/07 (PDT) </td>
                        <td> Hong Sun, Xue Li, Yinchuan Xu, Youkow Homma, Qi Cao, Min Wu, Jian Jiao and Denis Charles</td>
                        <td> Paper-5: AutoHint: Automatic Prompt Optimization with Hint Generation</td>
                    
                    </tr>

		    <tr>
                      <td>11:35-11:45AM, 2023/08/07 (PDT) </td>
                        <td>Zhichao Wang, Mengyu Dai and Keld Lundgaard</td>
                        <td>Paper-6: Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation</td>
                    </tr>
		       
		      <tr>
                      <td>11:45-11:55AM, 2023/08/07 (PDT) </td>
                        <td>Long Hoang Dang, Thao Minh Le, Tu Minh Phuong and Truyen Tran</td>
                        <td>Paper-7: Compositional Prompting with Successive Decomposition for Multimodal Language Models</td>
                    </tr>

                    <tr>
 		      <tr>
                      <td>11:55AM-12:05PM, 2023/08/07 (PDT) </td>
                        <td>Zhen Guo, Yanwei Wang, Peiqi Wang and Shangdi Yu</td>
                        <td>Paper-8: Dr. LLaMA: Improving Small Language Models on PubMedQA via Generative Data Augmentation</td>
                    </tr>                       
                   
		       <td>12:05-12:15PM, 2023/08/07 (PDT) </td>
                        <td> Haopeng Zhang, Xiao Liu and Jiawei Zhang</td>
                        <td> Paper-9 : Extractive Summarization via ChatGPT for Faithful Summary Generation </td>
                 
                    </tr>

                <tr>
                        <td>12:15 - 12:25PM,  2023/08/07 (PDT)  
	
                        <td  style="text-align:center" colspan="2">Closing Remarks  </td>
                    </tr>

                </table> 
      </div>
     </div>
      </div>
</section>


	
 <section id="section-organizers" class="section-wrapper team gray-bg">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <div class="section-title">
                        <h1>WORKSHOP ORGANIZERS</h1>
                    </div>
                </div>
            </div>

<div class="row">
                <div class="col-md-12">
                    <div class="section-title">
                        <h3>General Chairs</h3>
                    </div>
                </div>
            </div>

		  <div class="col-sm-3 col-md-3">
                    <figure class="thumbnail">
                        <a href="https://sites.google.com/view/edchi/" target="view_window"><img src="img/Ed.jpeg" class="img-responsive" alt="Image"></a>
                        <figcaption class="caption text-center">
                            <h4><small>Ed H. Chi</small></h4>
                            <h5>
                                <small>Google Research</small>
                            </h5>
                            <ul class="social-links">
                                <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                                <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                                <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                                <li><a href="#"><i class="fa fa-envelope"></i></a></li>
                            </ul>
                        </figcaption>
                    </figure>
                </div>
	          <div class="col-sm-3 col-md-3">
                    <figure class="thumbnail">
                        <a href="https://wei-research.github.io/" target="view_window"><img src="https://wei-research.github.io/assets/images/WeiLiu_Photo.jpg"" class="img-responsive" alt="Image"></a>
                        <figcaption class="caption text-center">
                            <h4><small>Wei Liu</small>
                            </h4>
                            <h5><small>University of Technology Sydney</small></h5>
                            <ul class="social-links">
                                <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                                <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                                <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                                <li><a href="#"><i class="fa fa-envelope"></i></a></li>
                            </ul>
                        </figcaption>
                    </figure>
                </div>
		    
                <div class="col-sm-3 col-md-3">
                    <figure class="thumbnail">
                        <a href="https://niteshchawla.nd.edu//" target="view_window"><img src="img/nitesh_chawla.jpg" class="img-responsive" alt="Image"></a>
                        <figcaption class="caption text-center">
                            <h4><small>Nitesh Chawla</small>
                            </h4>
                            <h5><small>University of Notre Dame</small></h5>
                            <ul class="social-links">
                                <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                                <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                                <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                                <li><a href="#"><i class="fa fa-envelope"></i></a></li>
                            </ul>
                        </figcaption>
                    </figure>
                </div>
                <div class="col-sm-3 col-md-3">
                    <figure class="thumbnail">
                        <a href="https://people.eng.unimelb.edu.au/baileyj/" target="view_window"><img src="img/jbailey.jpeg" class="img-responsive" alt="Image"></a>
                        <figcaption class="caption text-center">
                            <h4><small>James Bailey</small>
                            </h4>
                            <h5><small>The University of Melbourne</small></h5>
                            <ul class="social-links">
                                <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                                <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                                <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                                <li><a href="#"><i class="fa fa-envelope"></i></a></li>
                            </ul>
                        </figcaption>
                    </figure>
                </div>

		<div class="row">
                <div class="col-md-12">
                    <div class="section-title">
                        <h3>Program Committee Chairs</h3>
                    </div>
                </div>
            </div>
		
          <div class="col-sm-3 col-md-3">
                    <figure class="thumbnail">
                        <a href="https://www.linkedin.com/in/linsey-pang-50590450/" target="view_window"><img src="img/xlp.jpeg" class="img-responsive" alt="Image"></a>
                        <figcaption class="caption text-center">
                            <h4><small>Linsey Pang</small>
                            </h4>
                            <h5><small>Salesforce</small></h5>
                            <ul class="social-links">
                                <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                                <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                                <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                                <li><a href="#"><i class="fa fa-envelope"></i></a></li>
                            </ul>
                        </figcaption>
                    </figure>
                </div>
		
            <div class="row">
                <div class="col-sm-3 col-md-3">
                    <figure class="thumbnail">
                        <a href="https://www.linkedin.com/in/derekzcheng/" target="view_window"><img src="img/derek.jpeg" class="img-responsive" alt="Image"></a>
                        <figcaption class="caption text-center">
                            <h4><small>Derek Cheng</small>
                            </h4>
                            <h5>
                                <small> Google Research</small></h5>
                            <ul class="social-links">
                                <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                                <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                                <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                                <li><a href="#"><i class="fa fa-envelope"></i></a></li>
                            </ul>
                        </figcaption>
                    </figure>
                </div>
                <!-- /.col-sm-6 -->

                <div class="col-sm-3 col-md-3">
                    <figure class="thumbnail">
                        <a href="https://www.linkedin.com/in/dhaval-patel-2b287033/" target="view_window"><img src="img/dhaval.jpeg" class="img-responsive" alt="Image"></a>
                        <figcaption class="caption text-center">
                            <h4><small>Dhaval Patel</small>
                            </h4>
                            <h5><small>IBM Research</small></h5>
                            
                            <ul class="social-links">
                                <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                                <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                                <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                                <li><a href="#"><i class="fa fa-envelope"></i></a></li>
                            </ul>
                        </figcaption>
                    </figure>
                </div>
                
               
                <div class="col-sm-3 col-md-3">
                    <figure class="thumbnail">
                        <a href="https://research.ibm.com/people/sameep-mehta" target="view_window"><img src="img/sameep.jpeg" class="img-responsive" alt="Image"></a>
                        <figcaption class="caption text-center">
                            <h4><small>Sameep Mehta</small>
                            </h4>
                            <h5><small>IBM Research</small></h5>
                            <ul class="social-links">
                                <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                                <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                                <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                                <li><a href="#"><i class="fa fa-envelope"></i></a></li>
                            </ul>
                        </figcaption>
                    </figure>
                </div>   
<!-- 		<div class="col-sm-3 col-md-3">
                    <figure class="thumbnail">
                        <a href="https://www.linkedin.com/in/kexinxie/" target="view_window"><img src="img/kexin.jpeg" class="img-responsive" alt="Image"></a>
                        <figcaption class="caption text-center">
                            <h4><small>Kexin Xie</small></h4>
                            <h5>
                                <small>Salesforce</small>
                            </h5>
                            <ul class="social-links">
                                <li><a href="#"><i class="fa fa-facebook"></i></a></li>
                                <li><a href="#"><i class="fa fa-twitter"></i></a></li>
                                <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
                                <li><a href="#"><i class="fa fa-envelope"></i></a></li>
                            </ul>
                        </figcaption>
                    </figure>
                </div> -->

		</div>
		    

		<div class="row">
			
                <div class="col-sm-2 col-md-2">
<!--                     <figure class="thumbnail">
                        <a href="https://faculty.washington.edu/ankurt/" target="view_window"><img src="img/ankur.jpeg" class="img-responsive" alt="Image"></a>
                    </figure> -->
                </div>

		
	</div>

            </div>

            <!--/div-->
	
		    
		<p> <center> <strong>Contact: llmai.workshop@gmail.com </strong></center></p>

   
	</div>
    </section>


<section id="pc" class="section-wrapper team white-bg">
		   <div class="container">
			   <div class="row">
                <div class="col-md-12">
                    <div class="section-title">
                        <h1>Program Committee</h1>
                    </div>
                        <p> Chidansh Bhatt, IBM RESEARCH</p>
			<p> Wang-Cheng Kang, Google </p>
			<p> Ruoxi Wang, Google</p>
			<p> Hima Patel, IBM RESEARCH</p>
			<p> Abhishek Malvankar, IBM RESEARCH</p>
			<p> Jianmo Ni , Google </p>
	                <p> Mengyu Dai, Salesforce</p>
			<p> Zhichao Wang , Georgia Institute of Technology</p>
			<p> XueLi, Microsoft</p>
			<p> Sandeep Singh Sandha, Abacus.AI </p>
			<p> Sahisnu Mazumder, Intel Labs</p>
		   </div>
	 </div>
            </div>

	</section>

    

    <footer class="footer">
        <div class="copyright-section">
            <div class="container">
                <div class="col-md-12">
                    <div class="copytext text-center">&copy; Conference. All rights reserved | Website Organized By: <a href="https://github.com/Connorlee487">Connor Lee, Albert Wang</a> | Template By: <a href="https://uicookies.com">uiCookies</a></div>
                </div>
            </div>
            <!-- .container -->
        </div>
        <!-- .copyright-section -->
    </footer>
    <!-- .footer -->

</div>
<!-- #main-wrapper -->


<!-- jquery -->
<script src="js/jquery-2.1.4.min.js"></script>

<!-- Bootstrap -->
<script src="js/bootstrap.min.js"></script>

<!-- Plugin JavaScript -->
<script src="js/jquery.easing.min.js"></script>

<!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/ -->
<script src="https://maps.googleapis.com/maps/api/js"></script>

<!--<script src="js/one-page-nav.js"></script>-->
<script src="js/scripts.js"></script>
</body>
</html>
